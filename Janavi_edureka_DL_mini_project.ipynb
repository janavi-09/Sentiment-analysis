{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UhR9A2s4U2of",
    "outputId": "b89bd585-06ae-47d3-85c6-57d883be13fb"
   },
   "outputs": [],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yks4WYgLbdqo"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "yPG_0lBjb14u",
    "outputId": "17c3f47b-1c14-4140-94c0-e5d01ddd80d4"
   },
   "outputs": [],
   "source": [
    "twit = pd.read_csv(r\"C:\\Users\\sujee\\OneDrive\\Desktop\\Dataset\\Twitter_Data.csv\")\n",
    "twit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xCmdoDhITk9n"
   },
   "outputs": [],
   "source": [
    "# Change dependent variable to categorical\n",
    "twit['category'] = twit['category'].map({0.0: 'Neutral', -1.0: 'Negative', 1.0: 'Positive'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "YCLUG7kvT69d",
    "outputId": "1c23d1ef-a37f-41b3-d4d9-504378f806d4"
   },
   "outputs": [],
   "source": [
    "twit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uMMscD1JUAgU"
   },
   "outputs": [],
   "source": [
    "# Drop null/missing values\n",
    "twit.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H_jh_T5oWF5l",
    "outputId": "ddcdf8a4-bf27-4009-c36a-fe95782f2812"
   },
   "outputs": [],
   "source": [
    "twit.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CDxoVZ66UX3t"
   },
   "outputs": [],
   "source": [
    "# Text cleaning function\n",
    "def clean_text(text):\n",
    "    # Remove symbols except alphanumeric\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    # Transform to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_tokens = word_tokenize(text)\n",
    "    filtered_text = [word for word in word_tokens if word not in stop_words]\n",
    "    return ' '.join(filtered_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UmwYPL0CVH2Q",
    "outputId": "88babead-64b1-4d23-f081-f560d65c48d3"
   },
   "outputs": [],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "loosdIarV1AR",
    "outputId": "87f6f016-5e6b-421b-9454-f13de01c1694"
   },
   "outputs": [],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NdlxyGKEUdI0"
   },
   "outputs": [],
   "source": [
    "# Apply text cleaning\n",
    "twit['cleaned_text'] = twit['clean_text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YY3caSdmWmvn"
   },
   "outputs": [],
   "source": [
    "twit.drop('clean_text', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "31f4_vpiUr6o"
   },
   "outputs": [],
   "source": [
    "# Create a new column for the length of each sentence\n",
    "twit['sentence_length'] = twit['cleaned_text'].apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "30PFeWQfWhjR",
    "outputId": "c42f2beb-f8fe-4245-e109-78130a8fd2a4"
   },
   "outputs": [],
   "source": [
    "twit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "v690YR1yWkJz",
    "outputId": "4447a157-b8fe-4c0a-fc2f-1e095d0721ed"
   },
   "outputs": [],
   "source": [
    "twit = twit[['cleaned_text','sentence_length','category']]\n",
    "twit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fu_4qswXW9Hl"
   },
   "outputs": [],
   "source": [
    "# Split data into dependent (X) and independent (y) dataframes\n",
    "X = twit['cleaned_text']\n",
    "y = twit['category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iPrgxClAXLZR"
   },
   "outputs": [],
   "source": [
    "# Do one-hot encoding for each sentence\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X)\n",
    "X_encoded = tokenizer.texts_to_sequences(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OwXfEaBnXQot"
   },
   "outputs": [],
   "source": [
    "# Add padding from the front side\n",
    "max_sequence_length = max(twit['sentence_length'])\n",
    "X_padded = pad_sequences(X_encoded, maxlen=max_sequence_length, padding='pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wKEZ2oiSXm__"
   },
   "outputs": [],
   "source": [
    "# Build LSTM model\n",
    "vocabulary_size = len(tokenizer.word_index) + 1\n",
    "embedding_dim = 128\n",
    "lstm_units = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tdv5cD0pXuQf"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocabulary_size, embedding_dim, input_length=max_sequence_length),\n",
    "    tf.keras.layers.LSTM(lstm_units),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(3, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xXqF9jjYXwSa"
   },
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2O667RMlX01E"
   },
   "outputs": [],
   "source": [
    "# Do dummy variable creation for the dependent variable\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "y_categorical = tf.keras.utils.to_categorical(y_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KqFA4xzPX6ZF"
   },
   "outputs": [],
   "source": [
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_padded, y_categorical, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A1_EERdVX-cb",
    "outputId": "56c2567c-a688-478c-d8fb-040dc4c1317b"
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "training = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ljq08_xrYFwG"
   },
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "y_pred_prob = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize predictions\n",
    "y_pred = np.round(y_pred_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert predictions back to original categories\n",
    "y_pred_category = label_encoder.inverse_transform(np.argmax(y_pred, axis=1))\n",
    "y_true_category = label_encoder.inverse_transform(np.argmax(y_test, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print classification report\n",
    "print(classification_report(y_true_category, y_pred_category))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
